[{"paper_title": "Reevaluating Loss Functions: Enhancing Robustness to Label Noise in Deep Learning Models", "paper_summary": "Large annotated datasets inevitably contain incorrect labels, and deep neural networks can overfit to these mistakes, hurting generalization.  To counter this, researchers propose loss functions that are robust to label noise, but the space of such losses is wide, many require tuning, and some learn more slowly than the standard Cross\u2011Entropy.  In this study the authors systematically evaluate a broad set of existing robust losses, using heuristic arguments and extensive experiments to map out the situations where each performs best.  They introduce a simple yet effective tweak\u2014adding a small bias to the neuron pre\u2011activation of the correct class\u2014which dramatically improves bounded losses.  With this bias, the Mean Absolute Error loss even surpasses Cross\u2011Entropy on the clean Cifar\u2011100 benchmark.  Additionally, they design a new \u201cBounded Cross\u2011Entropy\u201d loss that inherits the stability of bounded losses while retaining the familiar form of Cross\u2011Entropy.  The paper therefore offers concrete guidance for selecting and improving loss functions in noisy\u2011label settings and demonstrates that bounded losses can be advantageous even when noise is minimal.", "relevance": "This work clarifies how to choose loss functions when training with noisy labels, a common challenge in real\u2011world data collection.  By showing that simple bias adjustments can revive bounded losses and that a new hybrid loss performs well, it opens avenues for more reliable deep learning pipelines, especially in domains where label quality cannot be guaranteed.  Future research can explore adaptive bias schedules, theoretical guarantees for bounded losses, and transfer to other tasks such as semi\u2011supervised learning or weak supervision.", "related_topics": ["Label noise robustness", "Robust loss functions", "Deep learning generalization"], "url": "https://arxiv.org/abs/2306.05497v1"}]