[{"paper_title": "Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism", "paper_summary": "The paper tackles the training challenges of Mixture-of-Experts (MoE) models, whose dynamic routing causes uneven expert loads and severe straggler effects when using traditional expert parallelism. It introduces Fully Sharded Sparse Data Parallelism (FSSDP), a paradigm that fully shards MoE parameters and optimizer states across devices, then rebuilds expert parameters on demand with two novel sparse collective operations: SparseAllGather and SparseReduceScatter. Building on FSSDP, the authors present Hecate, a MoE training system that combines heterogeneous sharding, sparse materialization, and re\u2011materialization techniques to create flexible expert placements that keep memory and communication overhead low. Experimental results demonstrate that Hecate can achieve up to 3.54\u00d7 speedup over state\u2011of\u2011the\u2011art MoE training systems while consistently improving across different model architectures and hardware platforms. The work shows that careful sparsity\u2011aware sharding and on\u2011the\u2011fly parameter construction can largely mitigate stragglers and enable scalable, cost\u2011effective training of large sparse models.", "relevance": "Efficient MoE training is critical for scaling next\u2011generation language models, where model sizes and data volumes keep growing. By addressing straggler effects and reducing memory and communication costs, Hecate unlocks MoE\u2019s potential on commodity multi\u2011GPU and multi\u2011node setups, making large sparse models more accessible. The techniques introduced\u2014especially sparse collectives and dynamic sharding\u2014open avenues for further research into adaptive load balancing and energy\u2011efficient distributed training. They also provide a concrete engineering foundation for deploying MoE architectures in production pipelines where latency and resource constraints are tight.", "related_topics": ["Mixture-of-Experts", "Sparse Data Parallelism", "Distributed Model Training"], "url": "https://arxiv.org/abs/2502.02581"}]