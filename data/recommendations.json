[{"paper_title": "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice", "paper_summary": "The paper analyzes how the distribution of singular values of a deep neural network\u2019s input\u2011output Jacobian affects training speed. Using tools from free probability, the authors derive the full singular value spectrum for deep networks with arbitrary nonlinearities, weight initializations, and depths. They show that ReLU activations cannot achieve dynamical isometry\u2014where all Jacobian singular values cluster near one\u2014while sigmoid activations can if the weights are initialized orthogonally. Empirical results demonstrate that networks initialized to satisfy dynamical isometry learn dramatically faster and outperform deep ReLU models on benchmark tasks. The work thus establishes a principled link between weight initialization, activation choice, and the conditioning of back\u2011propagation, offering a concrete design rule for training very deep networks.", "relevance": "By revealing that the entire Jacobian spectrum\u2014not just its mean\u2014governs learning dynamics, the paper highlights the importance of carefully controlling singular value distributions. This insight opens avenues for developing new initialization schemes and activation functions that achieve dynamical isometry, potentially extending the depth and efficiency of practical neural networks. Future work may explore these principles in convolutional, recurrent, or transformer architectures and investigate adaptive schemes that maintain isometry during training.", "related_topics": ["dynamical isometry", "orthogonal initialization", "Jacobian singular value distribution"], "url": "https://arxiv.org/abs/1711.04735"}, {"paper_title": "Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks", "paper_summary": "The paper investigates how the choice of weight initialization influences training speed in deep neural networks, focusing on a tractable deep linear model. The authors prove that initializing weights from the orthogonal group guarantees faster convergence compared to independent Gaussian initialization. Crucially, they show that with orthogonal initialization the network width required for efficient optimization does not grow with depth, whereas with Gaussian weights it scales linearly. This theoretical result explains why very deep nonlinear networks often benefit from orthogonal or dynamical\u2011isometry inspired initializations. The work provides a rigorous link between initialization geometry and learning dynamics, a connection that had been largely empirical until now. The proofs rely on spectral properties of random orthogonal matrices and careful analysis of gradient flow in linear layers. The findings suggest that a good initialization can sustain its advantage throughout learning, not just at the start.", "relevance": "Understanding the impact of initialization on convergence is vital for training deep networks, and this paper supplies a solid theoretical foundation for why orthogonal schemes work. The depth\u2011independent width requirement implies that very deep architectures can be trained efficiently without excessively increasing layer widths. Future work can extend these insights to nonlinear networks, explore other structured initializations, or design adaptive schemes that maintain dynamical isometry during training. Practitioners may adopt orthogonal initialization as a default, especially for depth\u2011heavy models, improving training stability and speed. The results open a research direction toward formalizing the role of spectral properties in deep learning optimization.", "related_topics": ["Orthogonal Initialization", "Deep Linear Networks", "Dynamical Isometry"], "url": "https://arxiv.org/abs/2001.05992"}, {"paper_title": "On the biological plausibility of orthogonal initialisation for solving gradient instability in deep neural networks", "paper_summary": "This paper tackles the long\u2011standing problem of vanishing and exploding gradients in deep neural networks by focusing on orthogonal weight initialization. While orthogonal matrices are known to mitigate gradient instability, they have been criticized as biologically implausible because they require sophisticated matrix\u2011factorization algorithms. The authors introduce two biologically plausible initialization schemes that let a network\u2019s weights gradually self\u2011organise into orthogonal matrices during training. They provide theoretical proof that pre\u2011training orthogonalisation always converges, ensuring the schemes are stable and reliable. Experiments on both recurrent and feedforward networks demonstrate that these methods outperform randomly initialized counterparts, yielding faster convergence and higher accuracy. The work thus bridges a gap between deep learning optimisation techniques and neurobiological plausibility, offering a practical and theoretically sound alternative to ad hoc orthogonal initialization.", "relevance": "By showing that orthogonal structures can arise through biologically realistic learning dynamics, the paper opens a pathway for developing neural architectures that are both efficient and biologically inspired. The convergence guarantees provide a solid foundation for further exploration of self\u2011organising weight structures in large\u2011scale networks. Future research can extend these schemes to convolutional or transformer\u2011style models and investigate their impact on generalisation and robustness. Additionally, the biologically plausible mechanisms may inspire new neuromorphic hardware designs that naturally maintain orthogonality, enhancing energy efficiency.", "related_topics": ["Orthogonal initialization", "Gradient stability in deep learning", "Biologically plausible learning rules"], "url": "https://arxiv.org/abs/2211.08408"}]