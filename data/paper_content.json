{"https://arxiv.org/abs/1711.04735": {"results": [{"url": "https://arxiv.org/abs/1711.04735", "title": "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice", "raw_content": "[1711.04735] Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n>cs> arXiv:1711.04735\nHelp | Advanced Search\nSearch\n\nGO\nquick links\n\nLogin\nHelp Pages\nAbout\n\nComputer Science > Machine Learning\narXiv:1711.04735 (cs)\n[Submitted on 13 Nov 2017]\nTitle:Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\nAuthors:Jeffrey Pennington, Samuel S. Schoenholz, Surya Ganguli\nView a PDF of the paper titled Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice, by Jeffrey Pennington and 2 other authors\nView PDF\n\nAbstract:It is well known that the initialization of weights in deep neural networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network's input-output Jacobian is O(1)is essential for avoiding the exponential vanishing or explosion of gradients. The stronger condition that all singular values of the Jacobian concentrate near 1is a property known as dynamical isometry. For deep linear networks, dynamical isometry can be achieved through orthogonal weight initialization and has been shown to dramatically speed up learning; however, it has remained unclear how to extend these results to the nonlinear setting. We address this question by employing powerful tools from free probability theory to compute analytically the entire singular value distribution of a deep network's input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning.\n\nComments:13 pages, 6 figures. Appearing at the 31st Conference on Neural Information Processing Systems (NIPS 2017)\nSubjects:Machine Learning (cs.LG); Machine Learning (stat.ML)\nCite as:arXiv:1711.04735 [cs.LG]\n(or arXiv:1711.04735v1 [cs.LG] for this version)\n\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Jeffrey Pennington [view email]\n[v1] Mon, 13 Nov 2017 18:06:09 UTC (1,341 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice, by Jeffrey Pennington and 2 other authors\n\nView PDF\nTeX Source\n\nview license\nCurrent browse context:\ncs.LG\n<prev | next>\nnew | recent | 2017-11\nChange to browse by:\ncs\nstat\nstat.ML\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nDBLP - CS Bibliography\nlisting | bibtex\nJeffrey Pennington\nSamuel S. Schoenholz\nSurya Ganguli\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by: \nBookmark\n\nBibliographic Tools\nBibliographic and Citation Tools\n\n[x] Bibliographic Explorer Toggle\n\nBibliographic Explorer (What is the Explorer?)\n\n[x] Connected Papers Toggle\n\nConnected Papers (What is Connected Papers?)\n\n[x] Litmaps Toggle\n\nLitmaps (What is Litmaps?)\n\n[x] scite.ai Toggle\n\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\n\n[x] alphaXiv Toggle\n\nalphaXiv (What is alphaXiv?)\n\n[x] Links to Code Toggle\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\n[x] DagsHub Toggle\n\nDagsHub (What is DagsHub?)\n\n[x] GotitPub Toggle\n\nGotit.pub (What is GotitPub?)\n\n[x] Huggingface Toggle\n\nHugging Face (What is Huggingface?)\n\n[x] Links to Code Toggle\n\nPapers with Code (What is Papers with Code?)\n\n[x] ScienceCast Toggle\n\nScienceCast (What is ScienceCast?)\nDemos\nDemos\n\n[x] Replicate Toggle\n\nReplicate (What is Replicate?)\n\n[x] Spaces Toggle\n\nHugging Face Spaces (What is Spaces?)\n\n[x] Spaces Toggle\n\nTXYZ.AI (What is TXYZ.AI?)\nRelated Papers\nRecommenders and Search Tools\n\n[x] Link to Influence Flower\n\nInfluence Flower (What are Influence Flowers?)\n\n[x] Core recommender toggle\n\nCORE Recommender (What is CORE?)\n\n[x] IArxiv recommender toggle\n\nIArxiv Recommender (What is IArxiv?)\n\nAuthor\nVenue\nInstitution\nTopic\n\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)\n\nAbout\n\nHelp\n\nContact\n\nSubscribe\n\nCopyright\n\nPrivacy Policy\n\nWeb Accessibility Assistance\n\narXiv Operational Status", "images": []}], "failed_results": [], "response_time": 2.99, "request_id": "58624123-ed00-41da-9238-9cfe1472f103"}, "https://arxiv.org/abs/2001.05992": {"results": [{"url": "https://arxiv.org/abs/2001.05992", "title": "Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks", "raw_content": "[2001.05992] Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n>cs> arXiv:2001.05992\nHelp | Advanced Search\nSearch\n\nGO\nquick links\n\nLogin\nHelp Pages\nAbout\n\nComputer Science > Machine Learning\narXiv:2001.05992 (cs)\n[Submitted on 16 Jan 2020]\nTitle:Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks\nAuthors:Wei Hu, Lechao Xiao, Jeffrey Pennington\nView a PDF of the paper titled Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks, by Wei Hu and 2 other authors\nView PDF\n\nAbstract:The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergence times and model performance. Yet despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes. In this work, we analyze the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. We show that for deep networks, the width needed for efficient convergence to a global minimum with orthogonal initializations is independent of the depth, whereas the width needed for efficient convergence with Gaussian initializations scales linearly in the depth. Our results demonstrate how the benefits of a good initialization can persist throughout learning, suggesting an explanation for the recent empirical successes found by initializing very deep non-linear networks according to the principle of dynamical isometry.\n\nComments:International Conference on Learning Representations (ICLR) 2020\nSubjects:Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Optimization and Control (math.OC); Machine Learning (stat.ML)\nCite as:arXiv:2001.05992 [cs.LG]\n(or arXiv:2001.05992v1 [cs.LG] for this version)\n\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Wei Hu [view email]\n[v1] Thu, 16 Jan 2020 18:48:34 UTC (121 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks, by Wei Hu and 2 other authors\n\nView PDF\nTeX Source\n\nview license\nCurrent browse context:\ncs.LG\n<prev | next>\nnew | recent | 2020-01\nChange to browse by:\ncs\ncs.NE\nmath\nmath.OC\nstat\nstat.ML\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nDBLP - CS Bibliography\nlisting | bibtex\nWei Hu\nLechao Xiao\nJeffrey Pennington\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by: \nBookmark\n\nBibliographic Tools\nBibliographic and Citation Tools\n\n[x] Bibliographic Explorer Toggle\n\nBibliographic Explorer (What is the Explorer?)\n\n[x] Connected Papers Toggle\n\nConnected Papers (What is Connected Papers?)\n\n[x] Litmaps Toggle\n\nLitmaps (What is Litmaps?)\n\n[x] scite.ai Toggle\n\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\n\n[x] alphaXiv Toggle\n\nalphaXiv (What is alphaXiv?)\n\n[x] Links to Code Toggle\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\n[x] DagsHub Toggle\n\nDagsHub (What is DagsHub?)\n\n[x] GotitPub Toggle\n\nGotit.pub (What is GotitPub?)\n\n[x] Huggingface Toggle\n\nHugging Face (What is Huggingface?)\n\n[x] Links to Code Toggle\n\nPapers with Code (What is Papers with Code?)\n\n[x] ScienceCast Toggle\n\nScienceCast (What is ScienceCast?)\nDemos\nDemos\n\n[x] Replicate Toggle\n\nReplicate (What is Replicate?)\n\n[x] Spaces Toggle\n\nHugging Face Spaces (What is Spaces?)\n\n[x] Spaces Toggle\n\nTXYZ.AI (What is TXYZ.AI?)\nRelated Papers\nRecommenders and Search Tools\n\n[x] Link to Influence Flower\n\nInfluence Flower (What are Influence Flowers?)\n\n[x] Core recommender toggle\n\nCORE Recommender (What is CORE?)\n\n[x] IArxiv recommender toggle\n\nIArxiv Recommender (What is IArxiv?)\n\nAuthor\nVenue\nInstitution\nTopic\n\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)\n\nAbout\n\nHelp\n\nContact\n\nSubscribe\n\nCopyright\n\nPrivacy Policy\n\nWeb Accessibility Assistance\n\narXiv Operational Status", "images": []}], "failed_results": [], "response_time": 1.64, "request_id": "f207b9ab-7704-4462-af05-0de744466c4c"}, "https://arxiv.org/abs/2211.08408": {"results": [{"url": "https://arxiv.org/abs/2211.08408", "title": "On the biological plausibility of orthogonal initialisation for solving gradient instability in deep neural networks", "raw_content": "[2211.08408] On the biological plausibility of orthogonal initialisation for solving gradient instability in deep neural networks\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n>cs> arXiv:2211.08408\nHelp | Advanced Search\nSearch\n\nGO\nquick links\n\nLogin\nHelp Pages\nAbout\n\nComputer Science > Neural and Evolutionary Computing\narXiv:2211.08408 (cs)\n[Submitted on 27 Oct 2022]\nTitle:On the biological plausibility of orthogonal initialisation for solving gradient instability in deep neural networks\nAuthors:Nikolay Manchev, Michael Spratling\nView a PDF of the paper titled On the biological plausibility of orthogonal initialisation for solving gradient instability in deep neural networks, by Nikolay Manchev and Michael Spratling\nView PDF\n\nAbstract:Initialising the synaptic weights of artificial neural networks (ANNs) with orthogonal matrices is known to alleviate vanishing and exploding gradient problems. A major objection against such initialisation schemes is that they are deemed biologically implausible as they mandate factorization techniques that are difficult to attribute to a neurobiological process. This paper presents two initialisation schemes that allow a network to naturally evolve its weights to form orthogonal matrices, provides theoretical analysis that pre-training orthogonalisation always converges, and empirically confirms that the proposed schemes outperform randomly initialised recurrent and feedforward networks.\n\nComments:9 pages, 3 figures, to be published in ISCMI2022 conference proceedings\nSubjects:Neural and Evolutionary Computing (cs.NE); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\nCite as:arXiv:2211.08408 [cs.NE]\n(or arXiv:2211.08408v1 [cs.NE] for this version)\n\nFocus to learn more\narXiv-issued DOI via DataCite\nRelated DOI:\nFocus to learn more\nDOI(s) linking to related resources\nSubmission history\nFrom: Nikolay Manchev [view email]\n[v1] Thu, 27 Oct 2022 06:08:06 UTC (547 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled On the biological plausibility of orthogonal initialisation for solving gradient instability in deep neural networks, by Nikolay Manchev and Michael Spratling\n\nView PDF\nTeX Source\n\nview license\nCurrent browse context:\ncs.NE\n<prev | next>\nnew | recent | 2022-11\nChange to browse by:\ncs\ncs.AI\ncs.LG\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by: \nBookmark\n\nBibliographic Tools\nBibliographic and Citation Tools\n\n[x] Bibliographic Explorer Toggle\n\nBibliographic Explorer (What is the Explorer?)\n\n[x] Connected Papers Toggle\n\nConnected Papers (What is Connected Papers?)\n\n[x] Litmaps Toggle\n\nLitmaps (What is Litmaps?)\n\n[x] scite.ai Toggle\n\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\n\n[x] alphaXiv Toggle\n\nalphaXiv (What is alphaXiv?)\n\n[x] Links to Code Toggle\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\n[x] DagsHub Toggle\n\nDagsHub (What is DagsHub?)\n\n[x] GotitPub Toggle\n\nGotit.pub (What is GotitPub?)\n\n[x] Huggingface Toggle\n\nHugging Face (What is Huggingface?)\n\n[x] Links to Code Toggle\n\nPapers with Code (What is Papers with Code?)\n\n[x] ScienceCast Toggle\n\nScienceCast (What is ScienceCast?)\nDemos\nDemos\n\n[x] Replicate Toggle\n\nReplicate (What is Replicate?)\n\n[x] Spaces Toggle\n\nHugging Face Spaces (What is Spaces?)\n\n[x] Spaces Toggle\n\nTXYZ.AI (What is TXYZ.AI?)\nRelated Papers\nRecommenders and Search Tools\n\n[x] Link to Influence Flower\n\nInfluence Flower (What are Influence Flowers?)\n\n[x] Core recommender toggle\n\nCORE Recommender (What is CORE?)\n\nAuthor\nVenue\nInstitution\nTopic\n\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)\n\nAbout\n\nHelp\n\nContact\n\nSubscribe\n\nCopyright\n\nPrivacy Policy\n\nWeb Accessibility Assistance\n\narXiv Operational Status", "images": []}], "failed_results": [], "response_time": 2.53, "request_id": "6e4eb840-d90e-4d05-914c-5f0575f5ce95"}}