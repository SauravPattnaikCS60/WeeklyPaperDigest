{"https://arxiv.org/abs/2502.06733": {"results": [{"url": "https://arxiv.org/abs/2502.06733", "title": "[2502.06733] Dynamic Loss-Based Sample Reweighting for Improved...", "raw_content": "We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\n\ncs > arXiv:2502.06733\n\nComputer Science > Machine Learning\narXiv:2502.06733 (cs)\n[Submitted on 10 Feb 2025]\nTitle:Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining\nAuthors:Daouda Sow, Herbert Woisetschl\u00e4ger, Saikiran Bulusu, Shiqiang Wang, Hans-Arno Jacobsen, Yingbin Liang\nView a PDF of the paper titled Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining, by Daouda Sow and 5 other authors\nView PDF HTML (experimental)\n\nAbstract:Pretraining large language models (LLMs) on vast and heterogeneous datasets is crucial for achieving state-of-the-art performance across diverse downstream tasks. However, current training paradigms treat all samples equally, overlooking the importance or relevance of individual samples throughout the training process. Existing reweighting strategies, which primarily focus on group-level data importance, fail to leverage fine-grained instance-level information and do not adapt dynamically to individual sample importance as training progresses. In this paper, we introduce novel algorithms for dynamic, instance-level data reweighting aimed at improving both the efficiency and effectiveness of LLM pretraining. Our methods adjust the weight of each training sample based on its loss value in an online fashion, allowing the model to dynamically focus on more informative or important samples at the current training stage. In particular, our framework allows us to systematically devise reweighting strategies deprioritizing redundant or uninformative data, which we find tend to work best. Furthermore, we develop a new theoretical framework for analyzing the impact of loss-based reweighting on the convergence of gradient-based optimization, providing the first formal characterization of how these strategies affect convergence bounds. We empirically validate our approach across a spectrum of tasks, from pretraining 7B and 1.4B parameter LLMs to smaller-scale language models and linear regression problems, demonstrating that our loss-based reweighting approach can lead to faster convergence and significantly improved performance.\n\nComments:\nAccepted for publication at ICLR 2025. Code base available: this https URL\n\nSubjects:\nMachine Learning (cs.LG); Artificial Intelligence (cs.AI)\n\nCite as:\narXiv:2502.06733 [cs.LG]\n\n(or  arXiv:2502.06733v1 [cs.LG] for this version)\n\n arXiv-issued DOI via DataCite\n\nSubmission history\nFrom: Herbert Woisetschl\u00e4ger [view email]\n[v1] Mon, 10 Feb 2025 17:57:15 UTC (1,455 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining, by Daouda Sow and 5 other authors\n\nView PDF\nHTML (experimental)\nTeX Source\n\nview license\nCurrent browse context:\ncs.LG\n< prev    |    next >\nnew  |  recent  | 2025-02\nChange to browse by:\ncs\ncs.AI\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by:\nBookmark\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nReplicate (What is Replicate?)\nHugging Face Spaces (What is Spaces?)\nTXYZ.AI (What is TXYZ.AI?)\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\nIArxiv Recommender (What is IArxiv?)\n\nAuthor\nVenue\nInstitution\nTopic\n\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)", "images": []}], "failed_results": [], "response_time": 0.02, "request_id": "16c00ebf-36f8-4f9e-9d23-8bd68c4602a3"}, "https://arxiv.org/abs/2204.05832": {"results": [{"url": "https://arxiv.org/abs/2204.05832", "title": "[2204.05832] What Language Model Architecture and Pretraining ...", "raw_content": "We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate\n\ncs > arXiv:2204.05832\n\nComputer Science > Computation and Language\narXiv:2204.05832 (cs)\n[Submitted on 12 Apr 2022]\nTitle:What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?\nAuthors:Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, Colin Raffel\nView a PDF of the paper titled What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?, by Thomas Wang and 7 other authors\nView PDF\n\nAbstract:Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at this https URL.\n\nSubjects:\nComputation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)\n\nCite as:\narXiv:2204.05832 [cs.CL]\n\n(or  arXiv:2204.05832v1 [cs.CL] for this version)\n\n arXiv-issued DOI via DataCite\n\nSubmission history\nFrom: Julien Launay [view email]\n[v1] Tue, 12 Apr 2022 14:19:49 UTC (4,292 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?, by Thomas Wang and 7 other authors\n\nView PDF\nTeX Source\n\nview license\nCurrent browse context:\ncs.CL\n< prev    |    next >\nnew  |  recent  | 2022-04\nChange to browse by:\ncs\ncs.LG\nstat\nstat.ML\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by:\nBookmark\nBibliographic and Citation Tools\nBibliographic Explorer (What is the Explorer?)\nConnected Papers (What is Connected Papers?)\nLitmaps (What is Litmaps?)\nscite Smart Citations (What are Smart Citations?)\nCode, Data and Media Associated with this Article\nalphaXiv (What is alphaXiv?)\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\nDagsHub (What is DagsHub?)\nGotit.pub (What is GotitPub?)\nHugging Face (What is Huggingface?)\nPapers with Code (What is Papers with Code?)\nScienceCast (What is ScienceCast?)\nDemos\nReplicate (What is Replicate?)\nHugging Face Spaces (What is Spaces?)\nTXYZ.AI (What is TXYZ.AI?)\nRecommenders and Search Tools\nInfluence Flower (What are Influence Flowers?)\nCORE Recommender (What is CORE?)\n\nAuthor\nVenue\nInstitution\nTopic\n\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)", "images": []}], "failed_results": [], "response_time": 0.02, "request_id": "02141444-973a-4b09-9732-8029ab73991b"}, "https://arxiv.org/abs/2410.14332v1": {"results": [{"url": "https://arxiv.org/abs/2410.14332v1", "title": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension", "raw_content": "[2410.14332v1] Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n>cs> arXiv:2410.14332v1\nHelp | Advanced Search\nSearch\n\nGO\nquick links\n\nLogin\nHelp Pages\nAbout\n\nComputer Science > Computer Vision and Pattern Recognition\narXiv:2410.14332v1 (cs)\n[Submitted on 18 Oct 2024 (this version), latest version 13 Aug 2025 (v4)]\nTitle:Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension\nAuthors:Yin Xie, Kaicheng Yang, Ninghua Yang, Weimo Deng, Xiangzi Dai, Tiancheng Gu, Yumeng Wang, Xiang An, Yongle Zhao, Ziyong Feng, Jiankang Deng\nView a PDF of the paper titled Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension, by Yin Xie and 10 other authors\nView PDFHTML (experimental)\n\nAbstract:Recent advances in Large Language Models (LLMs) have catalyzed the development of Large Multimodal Models (LMMs). However, existing research primarily focuses on tuning language and image instructions, ignoring the critical pretraining phase where models learn to process textual and visual modalities jointly. In this paper, we propose a new pretraining paradigm for LMMs to enhance the visual comprehension capabilities of LLMs by introducing a novel cross-modal comprehension stage. Specifically, we design a dynamically learnable prompt token pool and employ the Hungarian algorithm to replace part of the original visual tokens with the most relevant prompt tokens. Then, we conceptualize visual tokens as analogous to a \"foreign language\" for the LLMs and propose a mixed attention mechanism with bidirectional visual attention and unidirectional textual attention to comprehensively enhance the understanding of visual tokens. Meanwhile, we integrate a detailed caption generation task, leveraging rich descriptions to further facilitate LLMs in understanding visual semantic information. After pretraining on 1.5 million publicly accessible data, we present a new foundation model called Croc. Experimental results demonstrate that Croc achieves new state-of-the-art performance on massive vision-language benchmarks. To support reproducibility and facilitate further research, we release the training code and pre-trained model weights at this https URL.\n\nComments:18 pages, 11 figures\nSubjects:Computer Vision and Pattern Recognition (cs.CV)\nCite as:arXiv:2410.14332 [cs.CV]\n(or arXiv:2410.14332v1 [cs.CV] for this version)\n\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Kaicheng Yang [view email]\n[v1] Fri, 18 Oct 2024 09:44:25 UTC (3,620 KB)\n[v2] Sat, 30 Nov 2024 05:56:52 UTC (4,491 KB)\n[v3] Tue, 24 Dec 2024 03:27:45 UTC (4,490 KB)\n[v4] Wed, 13 Aug 2025 09:39:24 UTC (1,754 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension, by Yin Xie and 10 other authors\n\nView PDF\nHTML (experimental)\nTeX Source\n\nview license\nCurrent browse context:\ncs.CV\n<prev | next>\nnew | recent | 2024-10\nChange to browse by:\ncs\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by: \nBookmark\n\nBibliographic Tools\nBibliographic and Citation Tools\n\n[x] Bibliographic Explorer Toggle\n\nBibliographic Explorer (What is the Explorer?)\n\n[x] Connected Papers Toggle\n\nConnected Papers (What is Connected Papers?)\n\n[x] Litmaps Toggle\n\nLitmaps (What is Litmaps?)\n\n[x] scite.ai Toggle\n\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\n\n[x] alphaXiv Toggle\n\nalphaXiv (What is alphaXiv?)\n\n[x] Links to Code Toggle\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\n[x] DagsHub Toggle\n\nDagsHub (What is DagsHub?)\n\n[x] GotitPub Toggle\n\nGotit.pub (What is GotitPub?)\n\n[x] Huggingface Toggle\n\nHugging Face (What is Huggingface?)\n\n[x] Links to Code Toggle\n\nPapers with Code (What is Papers with Code?)\n\n[x] ScienceCast Toggle\n\nScienceCast (What is ScienceCast?)\nDemos\nDemos\n\n[x] Replicate Toggle\n\nReplicate (What is Replicate?)\n\n[x] Spaces Toggle\n\nHugging Face Spaces (What is Spaces?)\n\n[x] Spaces Toggle\n\nTXYZ.AI (What is TXYZ.AI?)\nRelated Papers\nRecommenders and Search Tools\n\n[x] Link to Influence Flower\n\nInfluence Flower (What are Influence Flowers?)\n\n[x] Core recommender toggle\n\nCORE Recommender (What is CORE?)\n\nAuthor\nVenue\nInstitution\nTopic\n\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)\n\nAbout\n\nHelp\n\nContact\n\nSubscribe\n\nCopyright\n\nPrivacy Policy\n\nWeb Accessibility Assistance\n\narXiv Operational Status", "images": []}], "failed_results": [], "response_time": 1.45, "request_id": "279055df-f658-4426-9c5c-7a77e4ccb651"}}