{"https://arxiv.org/abs/2502.02581": {"results": [{"url": "https://arxiv.org/abs/2502.02581", "title": "Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism", "raw_content": "[2502.02581] Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\n>cs> arXiv:2502.02581\nHelp | Advanced Search\nSearch\n\nGO\nquick links\n\nLogin\nHelp Pages\nAbout\n\nComputer Science > Distributed, Parallel, and Cluster Computing\narXiv:2502.02581 (cs)\n[Submitted on 4 Feb 2025]\nTitle:Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism\nAuthors:Yuhao Qing, Guichao Zhu, Fanxin Li, Lintian Lei, Zekai Sun, Xiuxian Guan, Shixiong Zhao, Xusheng Chen, Dong Huang, Sen Wang, Heming Cui\nView a PDF of the paper titled Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism, by Yuhao Qing and 10 other authors\nView PDFHTML (experimental)\n\nAbstract:Mixture-of-Experts (MoE) has emerged as a promising sparse paradigm for scaling up pre-trained models (PTMs) with remarkable cost-effectiveness. However, the dynamic nature of MoE leads to rapid fluctuations and imbalances in expert loads during training, resulting in significant straggler effects that hinder training performance when using expert parallelism (EP). Existing MoE training systems attempt to mitigate these effects through expert rearrangement strategies, but they face challenges in terms of memory efficiency and timeliness of rearrangement. This paper proposes Fully Sharded Sparse Data Parallelism (FSSDP), an innovative approach that tackles the parallelization of MoE layers and potential straggler effects caused by imbalanced expert loads from a new perspective. FSSDP fully shards the parameters and optimizer states of MoE layers across devices and sparsely materializes MoE parameters from scratch in each iteration with two sparse collectives SparseAllGather and SparseReduceScatter. We build Hecate, a high-performance MoE training system that incorporates FSSDP to fully unlock its potential. Hecate introduces heterogeneous sharding, sparse materialization, and re-materialization techniques to construct flexible and efficient expert placements with low memory and communication overhead. Our evaluation reveals that Hecate achieves up to 3.54x speedup compared over state-of-the-art MoE training systems and consistently demonstrates improvements across model architectures and hardware environments.\n\nSubjects:Distributed, Parallel, and Cluster Computing (cs.DC)\nCite as:arXiv:2502.02581 [cs.DC]\n(or arXiv:2502.02581v1 [cs.DC] for this version)\n\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Yuhao Qing [view email]\n[v1] Tue, 4 Feb 2025 18:56:00 UTC (803 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism, by Yuhao Qing and 10 other authors\n\nView PDF\nHTML (experimental)\nTeX Source\n\nview license\nCurrent browse context:\ncs.DC\n<prev | next>\nnew | recent | 2025-02\nChange to browse by:\ncs\nReferences & Citations\n\nNASA ADS\nGoogle Scholar\nSemantic Scholar\n\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by: \nBookmark\n\nBibliographic Tools\nBibliographic and Citation Tools\n\n[x] Bibliographic Explorer Toggle\n\nBibliographic Explorer (What is the Explorer?)\n\n[x] Connected Papers Toggle\n\nConnected Papers (What is Connected Papers?)\n\n[x] Litmaps Toggle\n\nLitmaps (What is Litmaps?)\n\n[x] scite.ai Toggle\n\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\n\n[x] alphaXiv Toggle\n\nalphaXiv (What is alphaXiv?)\n\n[x] Links to Code Toggle\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\n[x] DagsHub Toggle\n\nDagsHub (What is DagsHub?)\n\n[x] GotitPub Toggle\n\nGotit.pub (What is GotitPub?)\n\n[x] Huggingface Toggle\n\nHugging Face (What is Huggingface?)\n\n[x] Links to Code Toggle\n\nPapers with Code (What is Papers with Code?)\n\n[x] ScienceCast Toggle\n\nScienceCast (What is ScienceCast?)\nDemos\nDemos\n\n[x] Replicate Toggle\n\nReplicate (What is Replicate?)\n\n[x] Spaces Toggle\n\nHugging Face Spaces (What is Spaces?)\n\n[x] Spaces Toggle\n\nTXYZ.AI (What is TXYZ.AI?)\nRelated Papers\nRecommenders and Search Tools\n\n[x] Link to Influence Flower\n\nInfluence Flower (What are Influence Flowers?)\n\n[x] Core recommender toggle\n\nCORE Recommender (What is CORE?)\n\nAuthor\nVenue\nInstitution\nTopic\n\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?)\n\nAbout\n\nHelp\n\nContact\n\nSubscribe\n\nCopyright\n\nPrivacy Policy\n\nWeb Accessibility Assistance\n\narXiv Operational Status", "images": []}], "failed_results": [], "response_time": 3.25, "request_id": "7cbfc9f3-4e8f-44f1-9f8a-47699c4a8d6d"}}