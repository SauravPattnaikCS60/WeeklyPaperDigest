{"https://arxiv.org/abs/1802.08757": {"results": [{"url": "https://arxiv.org/abs/1802.08757", "title": "Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents", "raw_content": "[1802.08757] Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents\nSkip to main content\nImage 1: Cornell University Logo\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate\n\nImage 2: arxiv logo>cs> arXiv:1802.08757 \nHelp | Advanced Search\nSearch\nImage 3: arXiv logo\nImage 4: Cornell University Logo\nGO\nquick links\n   Login\n   Help Pages\n   About\nComputer Science > Machine Learning\narXiv:1802.08757 (cs) \nSubmitted on 23 Feb 2018 ([v1), last revised 27 Feb 2018 (this version, v2)]\nTitle:Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents\nAuthors:Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, Tamer Ba\u015far\nView a PDF of the paper titled Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents, by Kaiqing Zhang and 4 other authors\nView PDF\n> Abstract:We consider the problem of \\emph{fully decentralized} multi-agent reinforcement learning (MARL), where the agents are located at the nodes of a time-varying communication network. Specifically, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. Within this setting, the collective goal of the agents is to maximize the globally averaged return over the network through exchanging information with their neighbors. To this end, we propose two decentralized actor-critic algorithms with function approximation, which are applicable to large-scale MARL problems where both the number of states and the number of agents are massively large. Under the decentralized structure, the actor step is performed individually by each agent with no need to infer the policies of others. For the critic step, we propose a consensus update via communication over the network. Our algorithms are fully incremental and can be implemented in an online fashion. Convergence analyses of the algorithms are provided when the value functions are approximated within the class of linear functions. Extensive simulation results with both linear and nonlinear function approximations are presented to validate the proposed algorithms. Our work appears to be the first study of fully decentralized MARL algorithms for networked agents with function approximation, with provable convergence guarantees.\nSubjects:Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Optimization and Control (math.OC); Machine Learning (stat.ML)\nCite as:arXiv:1802.08757 [cs.LG]\n(or arXiv:1802.08757v2 [cs.LG] for this version)\n\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Kaiqing Zhang [view email] \n( Fri, 23 Feb 2018 22:53:32 UTC (1,867 KB)\n[v2] Tue, 27 Feb 2018 02:15:35 UTC (1,868 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents, by Kaiqing Zhang and 4 other authors\n   View PDF\n   TeX Source\nview license\nCurrent browse context: \ncs.LG\n<prev\") | next>\")\nnew | recent | 2018-02\nChange to browse by: \ncs\ncs.AI\ncs.MA\nmath\nmath.OC\nstat\nstat.ML\nReferences & Citations\n   NASA ADS\n   Google Scholar\n   Semantic Scholar\nDBLP - CS Bibliography\nlisting | bibtex\nKaiqing Zhang\nZhuoran Yang\nHan Liu\nTong Zhang\nTamer Basar\nexport BibTeX citation Loading...\nBibTeX formatted citation\n\u00d7\nData provided by: \nBookmark\nImage 5: BibSonomy logoImage 6: Reddit logo\nBibliographic Tools \nBibliographic and Citation Tools\n[x] Bibliographic Explorer Toggle \nBibliographic Explorer _(What is the Explorer?)_\n[x] Connected Papers Toggle \nConnected Papers _(What is Connected Papers?)_\n[x] Litmaps Toggle \nLitmaps _(What is Litmaps?)_\n[x] scite.ai Toggle \nscite Smart Citations _(What are Smart Citations?)_\nCode, Data, Media \nCode, Data and Media Associated with this Article\n[x] alphaXiv Toggle \nalphaXiv _(What is alphaXiv?)_\n[x] Links to Code Toggle \nCatalyzeX Code Finder for Papers _(What is CatalyzeX?)_\n[x] DagsHub Toggle \nDagsHub _(What is DagsHub?)_\n[x] GotitPub Toggle \nGotit.pub _(What is GotitPub?)_\n[x] Huggingface Toggle \nHugging Face _(What is Huggingface?)_\n[x] Links to Code Toggle \nPapers with Code _(What is Papers with Code?)_\n[x] ScienceCast Toggle \nScienceCast _(What is ScienceCast?)_\nDemos \nDemos\n[x] Replicate Toggle \nReplicate _(What is Replicate?)_\n[x] Spaces Toggle \nHugging Face Spaces _(What is Spaces?)_\n[x] Spaces Toggle \nTXYZ.AI _(What is TXYZ.AI?)_\nRelated Papers \nRecommenders and Search Tools\n[x] Link to Influence Flower \nInfluence Flower _(What are Influence Flowers?)_\n[x] Core recommender toggle \nCORE Recommender _(What is CORE?)_\n[x] IArxiv recommender toggle \nIArxiv Recommender _(What is IArxiv?)_\n   Author\n   Venue\n   Institution\n   Topic\nAbout arXivLabs  \narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\nWhich authors of this paper are endorsers? | Disable MathJax) (What is MathJax?) \n   About\n   Help\n   Contact\n   Subscribe\n   Copyright\n   Privacy Policy\n   Web Accessibility Assistance\n   arXiv Operational Status", "images": []}], "failed_results": [], "response_time": 1.42, "request_id": "3614c481-e2fd-4f44-9d78-6c69adfc7e17"}}